# 開発セッション記録 - 2025年11月14日

## セッション概要
AI脳処理システムの実装を完了。Hugging Faceを使った12GBモデルのホスティングと、GitHub Actionsでの自動AI推論システムを構築。

---

## 実施内容

### 1. AI脳処理システムのアーキテクチャ設計

#### システムフロー
```
1. Minecraftプラグイン
   ↓ brain.jsonを生成（チャット、視覚情報、メモリ）

2. brain.jsonをGitHubにプッシュ
   ↓ GitHub Actions自動起動

3. Hugging Faceからモデルダウンロード (12GB)
   ↓

4. AI推論実行
   - brain.jsonを読み込み
   - プロンプト生成
   - AI推論で次のタスクを決定
   ↓

5. brain.jsonのtasksセクションを更新
   ↓ 自動コミット・プッシュ

6. Minecraftプラグインが更新されたbrain.jsonを取得
   ↓

7. タスクを実行（木を掘る、石を掘る、移動など）
```

#### 設計上の課題と解決策

**課題**: 12GBのモデルファイルはGitHubに直接アップロード不可
- GitHubの制限: 通常ファイル100MB、Git LFS 2GB（無料枠）

**解決策**: Hugging Faceをモデルホスティングに使用
- 無料で大容量モデルをホスト可能
- GitHub Actionsとの相性が良い
- MLコミュニティで標準的

---

### 2. Python AI処理スクリプトの実装

#### ファイル構成
```
ai-brain/
├── brain_processor.py      # メイン処理スクリプト
├── prompt_template.py      # プロンプト生成とパース
└── requirements.txt        # Python依存関係
```

#### brain_processor.py
**機能**:
- GGUF形式のモデルを`llama-cpp-python`でロード
- brain.jsonからプロンプトを生成
- AI推論を実行（max_tokens: 128, temperature: 0.7）
- AIの応答をタスクオブジェクトにパース
- brain.jsonのtasksセクションに追加

**主要なパラメータ**:
```python
self.llm = Llama(
    model_path=str(self.model_path),
    n_ctx=4096,        # コンテキストサイズ
    n_threads=4,       # CPUスレッド数
    n_gpu_layers=-1,   # GPU使用（利用可能な場合）
    verbose=False
)

response = self.llm(
    prompt,
    max_tokens=128,    # 生成トークン数
    temperature=0.7,   # 創造性（0.0-1.0）
    top_p=0.9,         # Nucleus sampling
    stop=["\n"],       # 改行で停止
    echo=False
)
```

#### prompt_template.py
**機能**:
- brain.jsonからプロンプトを生成
- ボットの状態、チャット履歴、視覚情報、メモリを統合
- 7種類のタスクコマンドをサポート

**サポートするタスク**:
1. `MINE_WOOD x y z` - 木を掘る
2. `MINE_STONE x y z` - 石を掘る
3. `MOVE_TO x y z` - 移動
4. `CHAT <message>` - チャット送信
5. `GET_INVENTORY` - インベントリ取得
6. `GET_POSITION` - 位置取得
7. `WAIT` - 待機

**プロンプト構造**:
```
You are an AI controlling a Minecraft bot...

# RULES (Your core programming)
[ルール情報]

# CURRENT SITUATION

## Bot Position
X: 0, Y: 64, Z: 0
View Direction: Yaw 0.0, Pitch 0.0

## Recent Chat Messages
[最新10件のチャット]

## Visible Blocks (within your view)
[視界内のブロック20個]

## Memory (Important information you've learned)
[記憶情報]

# YOUR TASK
[タスクの説明とフォーマット]
```

---

### 3. Hugging Faceへのモデルアップロード

#### Hugging Faceアカウント設定
- **ユーザー名**: `imuzen127`
- **アクセストークン**: Write権限で作成
- **リポジトリ**: https://huggingface.co/imuzen127/gpt-oss-20b-GGUF

#### トークン設定の詳細
- **Token name**: `aiminer`
- **Type**: **Write** ← 読み書き権限が必須
- **Fine-grained access**: All repositories
- **Token expiration**: Never expire

**重要な学び**:
最初に作成したトークンが"Read"権限だったため、`401 Unauthorized`エラーが発生。"Write"権限で再作成して解決。

#### モデルアップロード
```bash
# ログイン
huggingface-cli login

# リポジトリ作成（Pythonスクリプトで実行）
from huggingface_hub import create_repo
create_repo(repo_id="imuzen127/gpt-oss-20b-GGUF", repo_type="model", private=False)

# モデルアップロード
huggingface-cli upload imuzen127/gpt-oss-20b-GGUF gpt-oss-20b-MXFP4.gguf
```

**アップロード結果**:
- ファイルサイズ: 11.28 GB
- アップロード時間: 約4分
- アップロード速度: 最大654MB/s（変動あり）
- URL: https://huggingface.co/imuzen127/gpt-oss-20b-GGUF/commit/bc12ae64c19b91d79e4a2434100997188e59f9e3

---

### 4. GitHub Actions ワークフローの実装

#### ワークフロー: `.github/workflows/ai-brain-process.yml`

**トリガー条件**:
1. brain.jsonがプッシュされた時
2. 手動実行（workflow_dispatch）

**実行ステップ**:
```yaml
1. Checkout repository
   - リポジトリをチェックアウト

2. Set up Python
   - Python 3.11をセットアップ

3. Install dependencies
   - ai-brain/requirements.txtから依存関係をインストール
   - llama-cpp-python, huggingface-hub

4. Download model from Hugging Face
   - HF_TOKENを使ってHugging Faceからモデルダウンロード
   - コマンド: huggingface-cli download imuzen127/gpt-oss-20b-GGUF gpt-oss-20b-MXFP4.gguf

5. Process brain with AI
   - brain.jsonが存在しない場合、サンプルを自動生成
   - brain_processor.pyを実行してAI推論
   - tasksセクションを更新

6. Commit updated brain
   - 変更されたbrain.jsonをコミット

7. Push changes
   - GitHubにプッシュ
```

#### GitHub Secretsの設定
- **Name**: `HF_TOKEN`
- **Value**: Hugging Faceのアクセストークン（Write権限）
- **用途**: Hugging Faceからのモデルダウンロード認証

---

### 5. ドキュメント作成

#### `huggingface_upload_guide.md`
**内容**:
- Hugging Faceアカウントの作成手順
- アクセストークンの詳細な設定方法
- モデルアップロードの2つの方法（CLI / Web UI）
- GitHub Secretsの設定手順
- トラブルシューティング

#### `upload_steps.md`
**内容**:
- 今すぐ実行できる簡易手順
- ステップバイステップのガイド
- 必要なコマンドのコピペ用テンプレート

---

## 成果物

### 実装されたファイル一覧

**AI処理システム** (3ファイル):
- `ai-brain/brain_processor.py` - メイン処理スクリプト（122行）
- `ai-brain/prompt_template.py` - プロンプト生成とパース（169行）
- `ai-brain/requirements.txt` - Python依存関係

**GitHub Actions** (1ファイル):
- `.github/workflows/ai-brain-process.yml` - ワークフロー定義（118行）

**ドキュメント** (2ファイル):
- `conversation-logs/huggingface_upload_guide.md` - 詳細ガイド
- `conversation-logs/upload_steps.md` - 簡易手順

---

## 技術的な詳細

### 使用技術

**Python環境**:
- Python 3.11
- llama-cpp-python 0.2.90 - GGUF形式のモデル実行
- huggingface-hub >= 0.20.0 - Hugging Face APIクライアント

**AI推論**:
- モデル形式: GGUF (GPT-OSS 20B, MXFP4量子化)
- 推論エンジン: llama.cpp (llama-cpp-pythonラッパー経由)
- コンテキストサイズ: 4096トークン
- 生成トークン数: 最大128トークン

**CI/CD**:
- GitHub Actions (ubuntu-latest)
- Hugging Face Hub (モデルホスティング)

---

## 技術的な課題と解決

### 課題1: 12GBモデルのGitHub管理
**問題**: GitHubは100MB以上のファイルを直接管理できない

**検討した解決策**:
1. Git LFS - 2GB制限があり、12GBは分割が必要
2. 外部サーバー（Google Drive等） - ダウンロード制限の問題
3. LM Studio API - GitHub Actionsからローカルにアクセス不可
4. **Hugging Face** ← 採用

**採用理由**:
- 無料で大容量モデルをホスト可能
- GitHub Actionsとの統合が容易
- MLコミュニティの標準プラットフォーム
- バージョン管理も可能

### 課題2: Hugging Faceトークン認証エラー
**問題**: `401 Unauthorized` エラーが発生

**原因**:
最初に作成したトークンが"Read"権限のみだった

**解決策**:
"Write"権限のトークンを再作成して解決

**学び**:
- Hugging Faceのトークンには"Read"と"Write"の2種類がある
- アップロードには"Write"権限が必須
- トークンは一度しか表示されないため、安全に保存する必要がある

### 課題3: GitHub Actionsでのモデルダウンロード時間
**懸念**: 12GBのダウンロードに時間がかかる可能性

**対策**:
- GitHub Actionsのタイムアウトを600000ms（10分）に設定
- Hugging Faceのネットワークは高速（実測で最大654MB/s）
- 将来的にはGitHub Actions Cacheでモデルをキャッシュ可能

**実測値（ローカル環境）**:
- ファイルサイズ: 11.28 GB
- アップロード時間: 約4分
- 平均速度: 約47MB/s〜654MB/s（変動あり）

---

## 現在の機能

### ✅ 動作する機能
1. **AI脳処理システム**
   - GGUF形式のモデルでAI推論実行
   - brain.jsonからプロンプトを自動生成
   - AIの応答をタスクにパース
   - tasksセクションを更新

2. **Hugging Face統合**
   - モデルの自動アップロード
   - モデルの自動ダウンロード
   - トークン認証

3. **GitHub Actions自動処理**
   - brain.json変更時の自動トリガー
   - モデルダウンロード
   - AI推論実行
   - 結果の自動コミット・プッシュ

4. **ドキュメント**
   - 詳細なセットアップガイド
   - トラブルシューティング

---

## 未実装の機能と今後の開発方針

### Phase 1: リアルタイム処理システムへの移行

**現在の問題点**:
- brain.jsonのファイルベース管理は非効率
- GitHubへのプッシュ→Actions起動→取得のラグがある
- リアルタイム性が低い

**理想のシステム**:
```
Minecraftプラグイン
    ↓ HTTP APIリクエスト
AIサーバー（常時起動）
    ↓ AI推論実行（即座に）
    ↓ HTTP レスポンス
Minecraftプラグイン
    ↓ タスク実行
```

**実装案**:
1. FastAPI/Flaskで軽量なAPIサーバーを構築
2. サーバーにモデルを常駐させる
3. MinecraftプラグインからHTTPリクエスト
4. 即座にAI推論を実行してレスポンス

**メリット**:
- レイテンシの大幅な削減（数秒→ミリ秒）
- ファイル管理が不要
- スケーラビリティの向上

**課題**:
- サーバーの常時起動が必要
- ホスティングコスト（無料枠の検討）
- ネットワークセキュリティ

### Phase 2: Minecraftプラグインとの統合

**必要な実装**:

1. **brain.json同期機能**
   - 定期的にbrain.jsonをGitHubにプッシュ
   - GitHubから更新されたbrain.jsonを取得（polling）
   - 変更検知とマージ処理

2. **タスク実行システム**
   - brain.jsonのtasksセクションを監視
   - タスクをデータパックコマンドに変換
   - コマンド実行とステータス更新

3. **視覚情報の実装**（前回セッションで未実装）
   - ボットエンティティの検出
   - 周囲のブロックスキャン（レイキャスト）
   - 視野角・遮蔽判定の実装

### Phase 3: モデルの柔軟な切り替え

**現在の仕組み**:
モデルは`.github/workflows/ai-brain-process.yml`でハードコード

**改善案**:
1. 環境変数でモデルを指定可能に
2. GitHub Secretsに`HF_MODEL_REPO`と`HF_MODEL_FILE`を追加
3. 複数のモデルをHugging Faceにアップロード
4. 用途に応じてモデルを切り替え

**モデル切り替え手順**:
```bash
# 新しいモデルをアップロード
huggingface-cli upload imuzen127/NEW_MODEL_NAME model.gguf

# GitHub Secretsを更新
HF_MODEL_REPO: imuzen127/NEW_MODEL_NAME
HF_MODEL_FILE: model.gguf

# ワークフローを更新
huggingface-cli download $HF_MODEL_REPO $HF_MODEL_FILE --local-dir ./models
```

### Phase 4: パフォーマンス最適化

**最適化ポイント**:

1. **モデルのキャッシュ**
   - GitHub Actions Cacheを使用
   - 初回ダウンロード後はキャッシュから読み込み
   - ビルド時間の短縮

2. **推論の高速化**
   - GPU対応（GitHub Actions有料プラン）
   - より小さいモデルの検討（精度とのトレードオフ）
   - バッチ処理の導入

3. **コスト最適化**
   - GitHub Actions実行時間の削減
   - 不要な処理の削減
   - トリガー条件の最適化

---

## AIモデルの入れ替え方法

### 方法1: 新しいモデルを別リポジトリにアップロード

```bash
# 新しいモデルをHugging Faceにアップロード
huggingface-cli upload imuzen127/NEW_MODEL_NAME model.gguf

# .github/workflows/ai-brain-process.ymlを編集
huggingface-cli download imuzen127/NEW_MODEL_NAME model.gguf --local-dir ./models

# brain_processor.pyの実行コマンドを更新
python brain_processor.py ../models/model.gguf "${BRAIN_PATH}"
```

### 方法2: 同じリポジトリに複数モデルを追加

```bash
# 複数のモデルファイルをアップロード
huggingface-cli upload imuzen127/gpt-oss-20b-GGUF model-small.gguf
huggingface-cli upload imuzen127/gpt-oss-20b-GGUF model-large.gguf

# 使いたいモデルを選択
huggingface-cli download imuzen127/gpt-oss-20b-GGUF model-small.gguf --local-dir ./models
```

### 方法3: 環境変数で動的に切り替え（推奨）

**GitHub Secretsに追加**:
- `HF_MODEL_REPO`: リポジトリ名
- `HF_MODEL_FILE`: モデルファイル名

**ワークフロー更新**:
```yaml
- name: Download model from Hugging Face
  env:
    HF_TOKEN: ${{ secrets.HF_TOKEN }}
    HF_MODEL_REPO: ${{ secrets.HF_MODEL_REPO || 'imuzen127/gpt-oss-20b-GGUF' }}
    HF_MODEL_FILE: ${{ secrets.HF_MODEL_FILE || 'gpt-oss-20b-MXFP4.gguf' }}
  run: |
    huggingface-cli download $HF_MODEL_REPO $HF_MODEL_FILE --local-dir ./models --token $HF_TOKEN

- name: Process brain with AI
  run: |
    python brain_processor.py ../models/${{ secrets.HF_MODEL_FILE }} "${BRAIN_PATH}"
```

---

## 参考情報

### Hugging Faceリポジトリ
- URL: https://huggingface.co/imuzen127/gpt-oss-20b-GGUF
- モデル: gpt-oss-20b-MXFP4.gguf
- サイズ: 11.28 GB
- コミット: bc12ae64c19b91d79e4a2434100997188e59f9e3

### GitHubリポジトリ
- URL: https://github.com/imuzen127/AIminer
- 主要なコミット:
  - `a32944e` - Add AI brain processing system with Hugging Face integration
  - `197ac72` - Enable Hugging Face model download in GitHub Actions workflow

### データパック側の既存コマンド（前回セッションより）
```mcfunction
# ボット召喚
/function imuzen127x74:summanekin  # 見た目召喚
/function imuzen127x74:sumpig      # 足召喚

# ボットの動作
/function imuzen127x74:xoak {x:-13,y:-55,z:47}     # 木を掘る
/function imuzen127x74:xstone {x:-3,y:-60,z:46}    # 石を掘る
/function imuzen127x74:xaim {x:-3,y:-60,z:48}      # 移動

# データ取得
/data get entity @e[tag=test1,limit=1] data
/data get entity @e[tag=test1,limit=1] data.Inventory
```

### 環境情報
- **Minecraft**: Paper 1.21
- **Java**: 21
- **Gradle**: 8.x
- **Python**: 3.11
- **OS**: Windows (MSYS_NT-10.0-26100)
- **IDE**: IntelliJ IDEA

---

## テスト方法

### 1. ローカルでのテスト

**前提**:
- モデルファイルがローカルに存在する
- Python環境がセットアップ済み

**手順**:
```bash
# 依存関係のインストール
pip install -r ai-brain/requirements.txt

# サンプルbrain.jsonを作成（ワークフローのサンプルを使用）

# AI推論を実行
cd ai-brain
python brain_processor.py "C:\Users\imuze\.lmstudio\models\lmstudio-community\gpt-oss-20b-GGUF\gpt-oss-20b-MXFP4.gguf" ../brain.json

# 結果を確認
cat ../brain.json
```

### 2. GitHub Actionsでのテスト

**手順**:
1. https://github.com/imuzen127/AIminer/actions にアクセス
2. "AI Brain Processing" ワークフローを選択
3. "Run workflow" をクリック
4. "Run workflow" ボタンをクリック
5. ログを確認

**確認ポイント**:
- モデルが正常にダウンロードされているか
- brain_processor.pyがエラーなく実行されているか
- brain.jsonのtasksセクションが更新されているか
- コミット・プッシュが成功しているか

### 3. 統合テスト（Minecraftプラグイン連携）

**手順**:
1. Minecraftサーバーを起動
2. プラグインを読み込み
3. ボットを召喚
4. プレイヤーがチャットで指示
5. プラグインがbrain.jsonを生成・GitHubにプッシュ
6. GitHub Actionsが自動実行
7. プラグインが更新されたbrain.jsonを取得
8. ボットがタスクを実行

**注意**:
統合テストには、前回セッションで未実装の「GitHub同期機能」と「タスク実行システム」の実装が必要。

---

## トラブルシューティング

### 問題1: GitHub Actionsでモデルダウンロードが失敗する

**エラーメッセージ**:
```
401 Unauthorized
```

**原因**:
- `HF_TOKEN`が設定されていない
- トークンの権限が不足している（Readのみ）

**解決策**:
1. GitHub Secretsで`HF_TOKEN`が正しく設定されているか確認
2. Hugging Faceトークンが**Write**権限を持っているか確認
3. 必要に応じて新しいWriteトークンを作成してSecretsを更新

### 問題2: AI推論が遅い

**原因**:
- CPUのみで推論している
- モデルが大きすぎる

**解決策**:
1. より小さいモデルを試す（例: 7B、13Bモデル）
2. 量子化レベルを下げる（MXFP4 → Q4_K_M → Q8_0）
3. GitHub Actions有料プランでGPUランナーを使用
4. ローカルサーバーでGPU推論を実行

### 問題3: brain.jsonのマージコンフリクト

**原因**:
- プラグインとGitHub Actionsが同時にbrain.jsonを更新

**解決策**:
1. ロック機構の実装（ファイルロックまたはブランチ戦略）
2. タイムスタンプベースのマージ戦略
3. リアルタイムAPIシステムへの移行（推奨）

---

## 次回セッションの予定

### 優先度1: ローカルテスト
1. サンプルbrain.jsonを作成
2. brain_processor.pyをローカルで実行
3. 出力されたtasksの確認
4. プロンプトのチューニング

### 優先度2: Minecraftプラグインとの統合
1. brain.jsonのGitHub同期機能の実装
2. タスク実行システムの実装
3. エンドツーエンドのテスト

### 優先度3: リアルタイムシステムへの移行
1. FastAPIサーバーの実装
2. MinecraftプラグインからのHTTPリクエスト
3. レイテンシの測定と最適化

---

## 学んだこと

### 技術的な学び
1. **Hugging Faceの使い方**
   - トークンの種類（ReadとWrite）
   - モデルのアップロード方法
   - GitHub Actionsとの統合

2. **llama-cpp-pythonの使い方**
   - GGUFモデルのロード
   - 推論パラメータの調整
   - GPUオフロードの設定

3. **GitHub Actionsのベストプラクティス**
   - Secretsの適切な使用
   - タイムアウトの設定
   - ワークフローのトリガー条件

### プロジェクト管理の学び
1. **段階的な開発の重要性**
   - まずローカルでテスト
   - 次にCI/CDで自動化
   - 最後に統合

2. **ドキュメントの重要性**
   - 詳細な手順書が再現性を高める
   - トラブルシューティングガイドが問題解決を加速

3. **柔軟な設計の重要性**
   - モデルの切り替えが容易
   - システムの段階的な改善が可能

---

## まとめ

今回のセッションでは、AI脳処理システムの基盤を完成させました。Hugging Faceを使った大容量モデルのホスティングと、GitHub Actionsでの自動AI推論システムを構築し、Minecraftボットの「脳」の部分が動作する環境が整いました。

次のステップとしては、Minecraftプラグイン側の実装を完成させ、エンドツーエンドで動作する自律型ボットシステムを実現することが目標です。また、将来的にはファイルベースの処理からリアルタイムAPIシステムへの移行を検討し、より実用的で応答性の高いシステムを目指します。

**現在の達成率**: 約60%
- ✅ 脳ファイルシステムの設計・実装（前回）
- ✅ AI脳処理システムの実装（今回）
- ⬜ Minecraftプラグインとの統合
- ⬜ タスク実行システム
- ⬜ エンドツーエンドテスト

引き続き開発を進めていきます。
